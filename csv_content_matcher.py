import os
import asyncio
import pandas as pd
import numpy as np
from itertools import cycle
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
import google.generativeai as genai
import time
import pickle
import logging
from datetime import datetime

# ===============================
# ãƒ­ã‚°è¨­å®š
# ===============================
os.makedirs("logs", exist_ok=True)
log_file = f"logs/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    filename=log_file,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", "%H:%M:%S")
console.setFormatter(formatter)
logging.getLogger().addHandler(console)

# ===============================
# åˆæœŸè¨­å®š
# ===============================
load_dotenv()

API_KEYS = [
    os.getenv("GEMINI_API_KEY_1"),
    os.getenv("GEMINI_API_KEY_2"),
    os.getenv("GEMINI_API_KEY_3"),
    os.getenv("GEMINI_API_KEY_4")
]
API_KEYS = [k for k in API_KEYS if k]
API_CYCLE = cycle(API_KEYS)
MODEL = "text-embedding-004"

CHUNK_SIZE = 500
OUTPUT_CSV = "match_results.csv"
B_EMBED_FILE = "b_embeddings.pkl"

# å„ã‚­ãƒ¼ã®åˆ©ç”¨ã‚«ã‚¦ãƒ³ã‚¿
KEY_USAGE = {key[-6:]: 0 for key in API_KEYS}

# ===============================
# Embeddingå–å¾—é–¢æ•°
# ===============================
def get_embedding(text, api_key, retries=3, delay=2):
    key_tail = api_key[-6:]
    for attempt in range(retries):
        try:
            genai.configure(api_key=api_key)
            result = genai.embed_content(model=MODEL, content=text)
            KEY_USAGE[key_tail] += 1
            return np.array(result["embedding"], dtype=np.float32)
        except Exception as e:
            logging.warning(f"[{key_tail}] Retry {attempt+1}/{retries} - {e}")
            time.sleep(delay)
    logging.error(f"[{key_tail}] Failed to embed after {retries} retries.")
    return None

# ===============================
# Bç¤¾ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ or ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ
# ===============================
def prepare_b_embeddings(b_csv_path):
    if os.path.exists(B_EMBED_FILE):
        logging.info("ğŸ“¦ Loading cached Bç¤¾ embeddings...")
        with open(B_EMBED_FILE, "rb") as f:
            data = pickle.load(f)
        return data["titles"], data["embeddings"]

    logging.info("âš™ï¸ Generating Bç¤¾ embeddings...")
    b_df = pd.read_csv(b_csv_path, header=None)
    b_titles = b_df[0].astype(str).tolist()
    b_embeddings = []
    for i, title in enumerate(tqdm(b_titles, desc="Embedding Bç¤¾")):
        key = next(API_CYCLE)
        emb = get_embedding(title, key)
        if emb is not None:
            b_embeddings.append(emb)
        else:
            b_embeddings.append(np.zeros(768))
    b_embeddings = np.vstack(b_embeddings)
    with open(B_EMBED_FILE, "wb") as f:
        pickle.dump({"titles": b_titles, "embeddings": b_embeddings}, f)
    return b_titles, b_embeddings

# ===============================
# Aç¤¾ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
# ===============================
async def process_chunk(a_list, b_titles, b_embeddings):
    loop = asyncio.get_event_loop()
    start = time.time()
    results = []

    a_embeddings = []
    for title in tqdm(a_list, desc="Embedding Aç¤¾", leave=False):
        key = next(API_CYCLE)
        emb = await loop.run_in_executor(None, get_embedding, title, key)
        if emb is not None:
            a_embeddings.append(emb)
        else:
            a_embeddings.append(np.zeros(b_embeddings.shape[1]))

    a_embeddings = np.vstack(a_embeddings)
    sims = cosine_similarity(a_embeddings, b_embeddings)

    for a_text, sim_row in zip(a_list, sims):
        top_idx = np.argmax(sim_row)
        top_score = sim_row[top_idx]
        results.append({
            "Aç¤¾ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": a_text,
            "Bç¤¾ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": b_titles[top_idx],
            "é¡ä¼¼åº¦": round(float(top_score), 4)
        })

    elapsed = time.time() - start
    avg_time = elapsed / len(a_list)
    logging.info(
        f"ğŸ§© Chunk done ({len(a_list)} items) | "
        f"Time: {elapsed:.1f}s | Avg/item: {avg_time:.2f}s"
    )
    return results

# ===============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
async def main(a_csv_path, b_csv_path):
    start_time = time.time()
    logging.info("===== E-Learning ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¯”è¼ƒé–‹å§‹ =====")

    # --- Bç¤¾æº–å‚™ ---
    b_titles, b_embeddings = prepare_b_embeddings(b_csv_path)

    # --- Aç¤¾èª­ã¿è¾¼ã¿ ---
    a_df = pd.read_csv(a_csv_path, header=None)
    a_titles = a_df[0].astype(str).tolist()

    # --- é€”ä¸­å†é–‹å¯¾å¿œ ---
    done_titles = set()
    if os.path.exists(OUTPUT_CSV):
        done_df = pd.read_csv(OUTPUT_CSV)
        done_titles = set(done_df["Aç¤¾ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"].tolist())
        logging.info(f"ğŸ”„ {len(done_titles)}ä»¶ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    remaining = [t for t in a_titles if t not in done_titles]
    total = len(remaining)
    logging.info(f"ğŸš€ æ®‹ã‚Š {total} ä»¶ã‚’å‡¦ç†é–‹å§‹ã—ã¾ã™ã€‚")

    for i in range(0, total, CHUNK_SIZE):
        chunk = remaining[i:i+CHUNK_SIZE]
        chunk_no = i // CHUNK_SIZE + 1
        logging.info(f"â¡ï¸ Chunk {chunk_no} é–‹å§‹ ({len(chunk)} ä»¶)")

        res = await process_chunk(chunk, b_titles, b_embeddings)

        # æ›¸ãè¾¼ã¿
        header_flag = not os.path.exists(OUTPUT_CSV)
        pd.DataFrame(res).to_csv(OUTPUT_CSV, mode="a", header=header_flag, index=False)

        elapsed_total = time.time() - start_time
        processed = i + len(chunk)
        speed = processed / elapsed_total
        est_total_time = total / speed if speed > 0 else 0
        eta = est_total_time - elapsed_total

        logging.info(
            f"âœ… Chunk {chunk_no} å®Œäº†ã€‚é€²æ— {processed}/{total} "
            f"({processed/total*100:.1f}%) | "
            f"é€Ÿåº¦: {speed:.2f}ä»¶/ç§’ | æ®‹ã‚Šæ¨å®š: {eta/60:.1f}åˆ†"
        )
        logging.info(
            "ğŸ”‘ APIä½¿ç”¨çŠ¶æ³: " + ", ".join([f"{k}:{v}" for k, v in KEY_USAGE.items()])
        )

    total_time = time.time() - start_time
    logging.info(f"ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ†")
    logging.info("ãƒ­ã‚°å‡ºåŠ›å…ˆ: " + log_file)

# ===============================
# å®Ÿè¡Œ
# ===============================
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: python compare_contents.py A_company.csv B_company.csv")
        exit(1)

    asyncio.run(main(sys.argv[1], sys.argv[2]))
