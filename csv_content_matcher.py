import os
import asyncio
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
import google.generativeai as genai
import time
import pickle
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# ===============================
# ãƒ­ã‚°è¨­å®š
# ===============================
os.makedirs("logs", exist_ok=True)
log_file = f"logs/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    filename=log_file,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", "%H:%M:%S")
console.setFormatter(formatter)
logging.getLogger().addHandler(console)

# ===============================
# åˆæœŸè¨­å®š
# ===============================
load_dotenv()

API_KEYS = [
    os.getenv("GEMINI_API_KEY_1"),
    os.getenv("GEMINI_API_KEY_2"),
    os.getenv("GEMINI_API_KEY_3"),
    os.getenv("GEMINI_API_KEY_4")
]
API_KEYS = [k for k in API_KEYS if k]
MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-004")

CHUNK_SIZE = int(os.getenv("INPUT1_CHUNK_SIZE", "500"))
OUTPUT_CSV = os.getenv("OUTPUT_CSV", "match_results.csv")
INPUT2_EMBED_FILE = "input2_embeddings.pkl"

# å„ã‚­ãƒ¼ã®åˆ©ç”¨ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
KEY_USAGE = {key[-6:]: 0 for key in API_KEYS}
USAGE_LOCK = asyncio.Lock()

# ===============================
# Embeddingå–å¾—é–¢æ•°
# ===============================
def get_embedding(text, api_key, retries=3, delay=2):
    key_tail = api_key[-6:]
    for attempt in range(retries):
        try:
            genai.configure(api_key=api_key)
            result = genai.embed_content(model=MODEL, content=text)
            return np.array(result["embedding"], dtype=np.float32), key_tail
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            else:
                logging.warning(f"[{key_tail}] Failed after {retries} retries: {e}")
    return None, key_tail

# ===============================
# å…¥åŠ›CSV2ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ or ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ
# ===============================
def prepare_input2_embeddings(input2_csv_path):
    if os.path.exists(INPUT2_EMBED_FILE):
        logging.info("ğŸ“¦ Loading cached å…¥åŠ›CSV2 embeddings...")
        with open(INPUT2_EMBED_FILE, "rb") as f:
            data = pickle.load(f)
        return data["items"], data["embeddings"]

    logging.info("âš™ï¸ Generating å…¥åŠ›CSV2 embeddings...")
    input2_df = pd.read_csv(input2_csv_path, header=None)
    input2_items = input2_df[0].astype(str).tolist()
    input2_embeddings = []
    
    # å…¥åŠ›CSV2ã¯ä»¶æ•°ãŒå°‘ãªã„ã®ã§1ã¤ã®ã‚­ãƒ¼ã§å‡¦ç†
    key = API_KEYS[0]
    for item in tqdm(input2_items, desc="Embedding å…¥åŠ›CSV2"):
        emb, _ = get_embedding(item, key)
        if emb is not None:
            input2_embeddings.append(emb)
        else:
            input2_embeddings.append(np.zeros(768))
    
    input2_embeddings = np.vstack(input2_embeddings)
    with open(INPUT2_EMBED_FILE, "wb") as f:
        pickle.dump({"items": input2_items, "embeddings": input2_embeddings}, f)
    return input2_items, input2_embeddings

# ===============================
# å…¥åŠ›CSV1ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆä¸¦åˆ—å¯¾å¿œç‰ˆï¼‰
# ===============================
async def process_chunk(chunk_id, input1_list, input2_items, input2_embeddings, api_key):
    start = time.time()
    results = []
    key_tail = api_key[-6:]

    logging.info(f"ğŸ”· Chunk {chunk_id} é–‹å§‹ ({len(input1_list)} ä»¶) [Key: {key_tail}]")

    # ä¸¦åˆ—ã§Embeddingå–å¾—
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor(max_workers=10) as executor:
        tasks = [
            loop.run_in_executor(executor, get_embedding, item, api_key)
            for item in input1_list
        ]
        embeddings_with_keys = await asyncio.gather(*tasks)

    # çµæœã‚’é›†è¨ˆ
    input1_embeddings = []
    for (emb, used_key), item in zip(embeddings_with_keys, input1_list):
        if emb is not None:
            input1_embeddings.append(emb)
            async with USAGE_LOCK:
                KEY_USAGE[used_key] = KEY_USAGE.get(used_key, 0) + 1
        else:
            input1_embeddings.append(np.zeros(input2_embeddings.shape[1]))

    input1_embeddings = np.vstack(input1_embeddings)
    sims = cosine_similarity(input1_embeddings, input2_embeddings)

    # é¡ä¼¼åº¦ã®é«˜ã„å…¥åŠ›CSV2é …ç›®ã‚’ç‰¹å®š
    for input1_text, sim_row in zip(input1_list, sims):
        top_idx = np.argmax(sim_row)
        top_score = sim_row[top_idx]
        results.append({
            "å…¥åŠ›CSV1é …ç›®å": input1_text,
            "å…¥åŠ›CSV2é …ç›®å": input2_items[top_idx],
            "é¡ä¼¼åº¦": round(float(top_score), 4)
        })

    elapsed = time.time() - start
    avg_time = elapsed / len(input1_list)
    logging.info(
        f"âœ… Chunk {chunk_id} å®Œäº† ({len(input1_list)} items) | "
        f"Time: {elapsed:.1f}s | Avg/item: {avg_time:.2f}s [Key: {key_tail}]"
    )
    
    return chunk_id, results

# ===============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
async def main(input1_csv_path, input2_csv_path):
    start_time = time.time()
    logging.info("===== CSVé …ç›®é¡ä¼¼åº¦æ¯”è¼ƒãƒ„ãƒ¼ãƒ« é–‹å§‹ =====")
    logging.info(f"ğŸ”‘ ä½¿ç”¨APIã‚­ãƒ¼æ•°: {len(API_KEYS)}")
    logging.info(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE}ä»¶")

    # --- å…¥åŠ›CSV2æº–å‚™ ---
    input2_items, input2_embeddings = prepare_input2_embeddings(input2_csv_path)

    # --- å…¥åŠ›CSV1èª­ã¿è¾¼ã¿ ---
    input1_df = pd.read_csv(input1_csv_path, header=None)
    input1_items = input1_df[0].astype(str).tolist()

    # --- é€”ä¸­å†é–‹å¯¾å¿œ ---
    done_items = set()
    if os.path.exists(OUTPUT_CSV):
        try:
            done_df = pd.read_csv(OUTPUT_CSV)
            # ã‚«ãƒ©ãƒ åã‚’ç¢ºèªï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
            if "å…¥åŠ›CSV1é …ç›®å" in done_df.columns:
                done_items = set(done_df["å…¥åŠ›CSV1é …ç›®å"].tolist())
            elif done_df.columns[0]:  # æœ€åˆã®ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
                done_items = set(done_df.iloc[:, 0].tolist())
            logging.info(f"ğŸ”„ {len(done_items)}ä»¶ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        except Exception as e:
            logging.warning(f"âš ï¸ æ—¢å­˜CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            logging.info("ğŸ’¡ æ—¢å­˜CSVã‚’å‰Šé™¤ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            logging.info("ğŸ”„ æœ€åˆã‹ã‚‰å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    remaining = [t for t in input1_items if t not in done_items]
    total = len(remaining)
    logging.info(f"ğŸš€ æ®‹ã‚Š {total} ä»¶ã‚’å‡¦ç†é–‹å§‹ã—ã¾ã™ã€‚")

    # ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ
    chunks = []
    for i in range(0, total, CHUNK_SIZE):
        chunk = remaining[i:i+CHUNK_SIZE]
        chunk_id = i // CHUNK_SIZE + 1
        chunks.append((chunk_id, chunk))

    # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ç”¨ã®ãƒ­ãƒƒã‚¯
    write_lock = asyncio.Lock()
    processed_count = 0

    # 4ã¤ãšã¤ä¸¦åˆ—å‡¦ç†
    num_workers = len(API_KEYS)
    for batch_start in range(0, len(chunks), num_workers):
        batch = chunks[batch_start:batch_start + num_workers]
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã«ç•°ãªã‚‹APIã‚­ãƒ¼ã‚’å‰²ã‚Šå½“ã¦
        tasks = []
        for idx, (chunk_id, chunk_data) in enumerate(batch):
            api_key = API_KEYS[idx % len(API_KEYS)]
            tasks.append(process_chunk(chunk_id, chunk_data, input2_items, input2_embeddings, api_key))
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks)
        
        # çµæœã‚’é †æ¬¡æ›¸ãè¾¼ã¿ï¼ˆãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆï¼‰
        results.sort(key=lambda x: x[0])
        for chunk_id, chunk_results in results:
            async with write_lock:
                header_flag = not os.path.exists(OUTPUT_CSV)
                pd.DataFrame(chunk_results).to_csv(
                    OUTPUT_CSV, mode="a", header=header_flag, index=False
                )
                processed_count += len(chunk_results)

        # é€²æ—è¡¨ç¤º
        elapsed_total = time.time() - start_time
        speed = processed_count / elapsed_total if elapsed_total > 0 else 0
        eta = (total - processed_count) / speed if speed > 0 else 0

        logging.info(
            f"ğŸ“Š é€²æ— {processed_count}/{total} "
            f"({processed_count/total*100:.1f}%) | "
            f"é€Ÿåº¦: {speed:.2f}ä»¶/ç§’ | æ®‹ã‚Šæ¨å®š: {eta/60:.1f}åˆ†"
        )
        async with USAGE_LOCK:
            logging.info(
                "ğŸ”‘ APIä½¿ç”¨çŠ¶æ³: " + ", ".join([f"{k}:{v}" for k, v in KEY_USAGE.items()])
            )

    total_time = time.time() - start_time
    logging.info(f"ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ†")
    logging.info(f"ğŸ“„ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {OUTPUT_CSV}")
    logging.info(f"ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")

# ===============================
# å®Ÿè¡Œ
# ===============================
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: python csv_content_matcher.py input1.csv input2.csv")
        exit(1)

    asyncio.run(main(sys.argv[1], sys.argv[2]))
